{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up environment"
      ],
      "metadata": {
        "id": "4KNaYcxCqn8t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqrW65lfqhhJ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git@main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "lTI8wKxgql9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the image captioning dataset\n",
        "\n",
        "Let's load the image captioning dataset, you just need few lines of code for that."
      ],
      "metadata": {
        "id": "mmUzlJNHq0CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ybelkada/football-dataset\", split=\"train\")"
      ],
      "metadata": {
        "id": "QwkaKQKLs8vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's retrieve the caption of the first example:"
      ],
      "metadata": {
        "id": "y52jgY9-mhvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "n4WLPzYlutg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the corresponding image"
      ],
      "metadata": {
        "id": "ukZ5x_Sumn1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"image\"]"
      ],
      "metadata": {
        "id": "lxoDUv5ymqBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch Dataset"
      ],
      "metadata": {
        "id": "sSWkkqiKqrhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lines below are entirely copied from the original notebook!"
      ],
      "metadata": {
        "id": "tBjZLhLFmwwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(images=item[\"image\"], text=item[\"text\"], padding=\"max_length\", return_tensors=\"pt\")\n",
        "        # remove batch dimension\n",
        "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "93od71o_qq_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and processor"
      ],
      "metadata": {
        "id": "VX6Nv8aEm0yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "hLhbdBLNxBuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have loaded the processor, let's load the dataset and the dataloader:"
      ],
      "metadata": {
        "id": "ZXlkVag6nDx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageCaptioningDataset(dataset, processor)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
      ],
      "metadata": {
        "id": "hxajSwc3w-LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "z_CyLSgBxyL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model! Run the simply the cell below for training the model"
      ],
      "metadata": {
        "id": "8cVsJdd2nV1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(50):\n",
        "  print(\"Epoch:\", epoch)\n",
        "  for idx, batch in enumerate(train_dataloader):\n",
        "    input_ids = batch.pop(\"input_ids\").to(device)\n",
        "    pixel_values = batch.pop(\"pixel_values\").to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids,\n",
        "                    pixel_values=pixel_values,\n",
        "                    labels=input_ids)\n",
        "\n",
        "    loss = outputs.loss\n",
        "\n",
        "    print(\"Loss:\", loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "6cCVhsmJxxjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "mNZQXZrERyQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the results on our train dataset"
      ],
      "metadata": {
        "id": "SiWpn-q1oFu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load image\n",
        "example = dataset[0]\n",
        "image = example[\"image\"]\n",
        "image"
      ],
      "metadata": {
        "id": "uC-Fp480XAt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare image for the model\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "pixel_values = inputs.pixel_values\n",
        "\n",
        "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
        "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(generated_caption)"
      ],
      "metadata": {
        "id": "6P3-u0eRxmsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][\"text\"]"
      ],
      "metadata": {
        "id": "WCeSg7Zj73pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][\"image\"]"
      ],
      "metadata": {
        "id": "NzfugtE77-wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[3][\"image\"]"
      ],
      "metadata": {
        "id": "O3_bvwpI9GqB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}